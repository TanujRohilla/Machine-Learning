{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import load_wine\n",
    "import sklearn.linear_model\n",
    "\n",
    "np.random.seed(1) # set a seed so that the results are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 178)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_wine()\n",
    "X = dataset.data\n",
    "Y = dataset.target\n",
    "\n",
    "#It is used to Generate Random Permutation\n",
    "idx = np.random.permutation(X.shape[0])\n",
    "X, Y = X[idx], Y[idx]\n",
    "\n",
    "X = X.T\n",
    "#Normalize data between 0 and 1\n",
    "X = (X-np.min(X, axis=1, keepdims = True))/(np.max(X, axis=1, keepdims = True)-np.min(X, axis = 1, keepdims = True))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Neural Network.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X(Features) is: (13, 178)\n",
      "The shape of Y(Target values) is: (1, 178)\n",
      "Number of training examples: 178\n"
     ]
    }
   ],
   "source": [
    "shape_X = X.shape\n",
    "Y = Y.reshape(1,Y.shape[0])\n",
    "shape_Y = Y.shape\n",
    "m = X.shape[1]\n",
    "\n",
    "#Converting Class 0 and 2 to class 0\n",
    "Y[Y != 1] = 0\n",
    "\n",
    "\n",
    "print ('The shape of X(Features) is: ' + str(shape_X))\n",
    "print ('The shape of Y(Target values) is: ' + str(shape_Y))\n",
    "print ('Number of training examples:', m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the input layer is: n_x = 13\n",
      "The size of the hidden layer 1 is: n_h = 4\n",
      "The size of the hidden layer 2 is: n_h = 2\n",
      "The size of the output layer is: n_y = 1\n"
     ]
    }
   ],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h1 -- the size of the hidden layer 1\n",
    "    n_h2 -- the size of the hidden layer 2\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h1 = 4 #Hidden Layer 1 contains 4 Nodes\n",
    "    n_h2 = 2 #Hidden Layer 2 contains 2 Nodes\n",
    "    n_y = Y.shape[0] \n",
    "    return (n_x, n_h1,n_h2, n_y)\n",
    "\n",
    "(n_x, n_h1, n_h2, n_y) = layer_sizes(X, Y)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the hidden layer 1 is: n_h = \" + str(n_h1))\n",
    "print(\"The size of the hidden layer 2 is: n_h = \" + str(n_h2))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h1, n_h2, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h1 -- size of the hidden layer 1\n",
    "    n_h2 -- size of the hidden layer 2\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h1, n_x) \n",
    "                    b1 -- bias vector of shape (n_h1, 1)\n",
    "                    W2 -- weight matrix of shape (n_h2,n_h1)\n",
    "                    b2 -- bias vector of shape (n_h2,1)\n",
    "                    W3 -- weight matrix of shape (n_y, n_h2)\n",
    "                    b3 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
    "    \n",
    "    \n",
    "    W1 = np.random.randn(n_h1,n_x)*0.005\n",
    "    b1 = np.zeros((n_h1,1))\n",
    "    W2 = np.random.randn(n_h2,n_h1)*0.005\n",
    "    b2 = np.zeros((n_h2,1))\n",
    "    W3 = np.random.randn(n_y,n_h2)*0.005\n",
    "    b3 = np.zeros((n_y,1))\n",
    "   \n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 13) (13, 178)\n",
      "(2, 4) (4, 13)\n",
      "(1, 2) (2, 4)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(n_x, n_h1,n_h2, n_y)\n",
    "print(parameters[\"W1\"].shape,X.shape)\n",
    "print(parameters[\"W2\"].shape,parameters[\"W1\"].shape)\n",
    "print(parameters[\"W3\"].shape,parameters[\"W2\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1.0/(1+np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the ReLu value of x\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "    \n",
    "    Return:\n",
    "    relu(x) = {0 if x < 0\n",
    "               x if x >= 0}\n",
    "    for all values of x\n",
    "    \"\"\"\n",
    "    \n",
    "    x[x<0] = 0\n",
    "    return x\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    s = np.log(1+np.exp(x))\n",
    "    return s\n",
    "\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A3 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\", \"Z3\", \"A3\"\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    #Using above defined functions to calculate A3 to implement Forward Propogation\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3,A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2,\n",
    "             \"Z3\": Z3,\n",
    "             \"A3\": A3}\n",
    "    \n",
    "    return A3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 178)\n"
     ]
    }
   ],
   "source": [
    "A3, cache = forward_propagation(X, parameters)\n",
    "print(A3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "    def compute_cost(A3, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A3 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply(np.log(A3), Y) + np.multiply((1 - Y), np.log(1 - A3))\n",
    "    cost = -np.sum(logprobs) / m\n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/krishna1401/Machine-Learning/blob/master/Activation%20Function.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: backward_propagation\n",
    "\n",
    "def relu_activation(x):\n",
    "    \"\"\"\n",
    "    ReLu Activation function for Backward Propogation\n",
    "    \n",
    "    Arguments: x numpy array of any size\n",
    "    \n",
    "    Returns: Derivative of the ReLu function on each element of x\n",
    "    relu_activation(x) = { 0 if x < 0\n",
    "                           1 if x >= 0}\n",
    "    \"\"\"\n",
    "    x[x < 0] = 0\n",
    "    x[x >= 0] = 1\n",
    "    return x\n",
    "\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\", \"A2\", \"Z3\" and \"A3\"\n",
    "    X -- input data of shape (13, number of examples)\n",
    "    Y -- \"0/1\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "        \n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    A3 = cache[\"A3\"]\n",
    "    \n",
    "    #Sigmoid Activation Function\n",
    "    dZ3 = A3-Y\n",
    "    dW3 = 1/m*(np.dot(dZ3,A2.T))\n",
    "    db3 = 1/m*(np.sum(dZ3, axis=1, keepdims=True))\n",
    "    \n",
    "    #ReLu Activation Function\n",
    "    dZ2 = np.multiply(np.dot(W3.T,dZ3),relu_activation(A2))\n",
    "    dW2 = 1/m*(np.dot(dZ2,A1.T))\n",
    "    db2 = 1/m*(np.sum(dZ2,axis=1, keepdims=True))\n",
    "    \n",
    "    #tanh Activation Function\n",
    "    dZ1 = np.multiply(np.dot(W2.T,dZ2),(1-np.power(A1,2)))\n",
    "    dW1 = 1/m*(np.dot(dZ1,X.T))\n",
    "    db1 = 1/m*(np.sum(dZ1,axis=1, keepdims=True))\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2,\n",
    "             \"dW3\": dW3,\n",
    "             \"db3\": db3}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 0.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    dW3 = grads[\"dW3\"]\n",
    "    db3 = grads[\"db3\"]\n",
    "    \n",
    "    #Updating all weights & bias using Learning Rate and Backward Propogation parameters\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    W3 = W3 - learning_rate*dW3\n",
    "    b3 = b3 - learning_rate*db3\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 1\n"
     ]
    }
   ],
   "source": [
    "n_x = layer_sizes(X, Y)[0]\n",
    "n_y = layer_sizes(X, Y)[3]\n",
    "print(n_x,n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: nn_model\n",
    "\n",
    "def nn_model(X, Y, n_h1, n_h2, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (3, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[3]\n",
    "    \n",
    "    parameters = initialize_parameters(n_x,n_h1,n_h2,n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    total_cost = 0\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A3, cache\".\n",
    "        A3, cache = forward_propagation(X,parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A3, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A3,Y,parameters)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters,cache,X,Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters,grads)\n",
    "        \n",
    "        #Total Cost of the complete Neural Network\n",
    "        total_cost += cost\n",
    "    \n",
    "    print(total_cost)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A3, cache = forward_propagation(X,parameters)\n",
    "    predictions = 1*(A3>0.5)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4956.720197843372\n",
      "[[0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1\n",
      "  1 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 0\n",
      "  0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0\n",
      "  0 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1\n",
      "  0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0]]\n",
      "(13, 178) (1, 178)\n",
      "Accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = nn_model(X, Y, n_h1 = 4, n_h2 = 2, num_iterations = 20000, print_cost=True)\n",
    "# Print accuracy\n",
    "predictions = predict(parameters, X)\n",
    "print(predictions)\n",
    "print( X.shape, predictions.shape)\n",
    "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
